{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c7AUZ3jMJY_"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirsazzathossain/CSE317-Lab/blob/autumn_2022/Lab_Assignment_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kAPQiFOHL9aw"
      },
      "source": [
        "#### **Polynomial Regression**\n",
        "\n",
        "In this assignment, you will implement polynomial regression and apply it to the [Assignment 4 Dataset](https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/mirsazzathossain/CSE317-Lab-Numerical-Methods/blob/main/datasets/data.csv).\n",
        "\n",
        "The dataset contains two columns, the first column is the feature and the second column is the label. The goal is find the best fit line for the data.\n",
        "\n",
        "You will need to perform the following regression tasks and find the best one for the dataset.\n",
        "\n",
        "1.    **Linear Regression:**\n",
        "\n",
        "     The equation we are trying to fit is:\n",
        "     $$y = \\theta_0 + \\theta_1 x$$\n",
        "     where $x$ is the feature and $y$ is the label.\n",
        "\n",
        "     We can rewrite the equation in vector form as:\n",
        "$$Y = X\\theta$$ where $X$ is a matrix with two columns, the first column is all 1s and the second column is the feature, and $Y$ is a vector with the labels. $\\theta$ is a vector with two elements, $\\theta_0$ and $\\theta_1$. The $X$ matrix will look like this:\n",
        "$$X = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}$$\n",
        "2. **Quadratic Regression:**\n",
        "\n",
        "     The equation we are trying to fit is:\n",
        "     $$y = \\theta_0 + \\theta_1 x + \\theta_2 x^2$$\n",
        "     where $x$ is the feature and $y$ is the label.\n",
        "\n",
        "     We can rewrite the equation in vector form as:\n",
        "$$Y = X\\theta$$where $X$ is a matrix with three columns, the first column is all 1s, the second column is the feature, and the third column is the feature squared, and $Y$ is a vector with the labels. $\\theta$ is a vector with three elements, $\\theta_0$, $\\theta_1$, and $\\theta_2$. The $X$ matrix will look like this:\n",
        "\n",
        "$$X = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix}$$\n",
        "3. **Cubic Regression:**\n",
        "\n",
        "     The equation we are trying to fit is:\n",
        "$$y = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3$$\n",
        "     where $x$ is the feature and $y$ is the label.\n",
        "\n",
        "     We can rewrite the equation in vector form as:\n",
        "$$Y = X\\theta$$where $X$ is a matrix with four columns, the first column is all 1s, the second column is the feature, the third column is the feature squared, and the fourth column is the feature cubed, and $Y$ is a vector with the labels. $\\theta$ is a vector with four elements, $\\theta_0$, $\\theta_1$, $\\theta_2$, and $\\theta_3$. The $X$ matrix will look like this:\n",
        "$$X = \\begin{bmatrix} 1 & x_1 & x_1^2 & x_1^3 \\\\ 1 & x_2 & x_2^2 & x_2^3 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 & x_n^3 \\end{bmatrix}$$\n",
        "\n",
        "Take 15 data points from the dataset and use them as the training set. Use the remaining data points as the test set. For each regression task, find the best $\\theta$ vector using the training set. Then, calculate the mean squared error (MSE) on the test set. Plot the training set, the test set (in a different color), and the best fit line for each regression task. Which regression task gives the best fit line? Which regression task gives the lowest MSE on the test set? Report your answers in a Markdown cell.\n",
        "\n",
        "**Note:** Do not use any built-in functions like `np.polyfit` or `sklearn.linear_model.LinearRegression` or any other built-in functions that perform polynomial regression. You must implement the regression tasks yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data.csv')\n",
        "#print(df.columns)\n",
        "\n",
        "features_column_index = 0\n",
        "labels_column_index = 1\n",
        "features = df.iloc[:, features_column_index].values\n",
        "labels = df.iloc[:, labels_column_index].values\n",
        "\n",
        "num_training_samples = 15\n",
        "X_train = features[:num_training_samples]\n",
        "y_train = labels[:num_training_samples]\n",
        "X_test = features[num_training_samples:]\n",
        "y_test = labels[num_training_samples:]\n",
        "\n",
        "# Step 2\n",
        "def linear_regression(X, y):\n",
        "    X_augmented = np.column_stack((np.ones_like(X), X))\n",
        "    theta = np.linalg.inv(X_augmented.T @ X_augmented) @ (X_augmented.T @ y)\n",
        "    return theta\n",
        "\n",
        "def quadratic_regression(X, y):\n",
        "    X_augmented = np.column_stack((np.ones_like(X), X, X ** 2))\n",
        "    theta = np.linalg.inv(X_augmented.T @ X_augmented) @ (X_augmented.T @ y)\n",
        "    return theta\n",
        "\n",
        "def cubic_regression(X, y):\n",
        "    X_augmented = np.column_stack((np.ones_like(X), X, X ** 2, X ** 3))\n",
        "    theta = np.linalg.inv(X_augmented.T @ X_augmented) @ (X_augmented.T @ y)\n",
        "    return theta\n",
        "\n",
        "# Step 3\n",
        "def calculate_mse(predictions, actual):\n",
        "    mse = np.mean((predictions - actual) ** 2)\n",
        "    return mse\n",
        "\n",
        "# Step 4\n",
        "def plot_regression_results(X_train, y_train, X_test, y_test, y_pred, label):\n",
        "    plt.scatter(X_train, y_train, label='Training Data')\n",
        "    plt.scatter(X_test, y_test, label='Test Data')\n",
        "    plt.plot(X_test, y_pred, label=label, color='red')\n",
        "    plt.xlabel('Feature')\n",
        "    plt.ylabel('Label')\n",
        "    plt.legend()\n",
        "    plt.title(label)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "regression_methods = [\n",
        "    (\"Linear Regression\", linear_regression),\n",
        "    (\"Quadratic Regression\", quadratic_regression),\n",
        "    (\"Cubic Regression\", cubic_regression)\n",
        "]\n",
        "\n",
        "for label, regression_method in regression_methods:\n",
        "    theta = regression_method(X_train, y_train)\n",
        "\n",
        "\n",
        "    if label == \"Linear Regression\":\n",
        "        X_pred = np.column_stack((np.ones_like(X_test), X_test))\n",
        "    elif label == \"Quadratic Regression\":\n",
        "        X_pred = np.column_stack((np.ones_like(X_test), X_test, X_test ** 2))\n",
        "    else:\n",
        "        X_pred = np.column_stack((np.ones_like(X_test), X_test, X_test ** 2, X_test ** 3))\n",
        "\n",
        "    y_pred = np.dot(X_pred, theta)\n",
        "    mse = calculate_mse(y_pred, y_test)\n",
        "    print(f\"{label}: MSE = {mse:.2f}\")\n",
        "    plot_regression_results(X_train, y_train, X_test, y_test, y_pred, label)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b148fc9bfa8b60132af830e32e1690e4e023b803e92912df15b823b90141dda6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
